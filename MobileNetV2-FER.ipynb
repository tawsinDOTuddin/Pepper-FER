{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f95e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import Model\n",
    "#from keras.optimizers import Adam\n",
    "#from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b83ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_generator = ImageDataGenerator(rotation_range=.15, \n",
    "                                     #brightness_range=[0.1, 0.7],\n",
    "                                     width_shift_range=0.1, \n",
    "                                     height_shift_range=0.1,\n",
    "                                     horizontal_flip=True, \n",
    "                                     #vertical_flip=True,\n",
    "                                     validation_split=0.20,\n",
    "                                     preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input) # VGG16 preprocessing\n",
    "\n",
    "test_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3615cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10480 images belonging to 10 classes.\n",
      "Found 2620 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = 'C:/Users/colorlab/Downloads/Emotion_Classes-20211030T160812Z-001/Emotion_Classes'\n",
    "class_subset = sorted(os.listdir(train_data_dir))[:10]\n",
    "traingen = train_generator.flow_from_directory(train_data_dir,\n",
    "                                               target_size=(48, 48),\n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_subset,\n",
    "                                               subset='training',\n",
    "                                               #color_mode = 'grayscale',\n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               shuffle=True,\n",
    "                                               seed=42)\n",
    "\n",
    "validgen = train_generator.flow_from_directory(train_data_dir,\n",
    "                                               target_size=(48, 48),\n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_subset,\n",
    "                                               subset='validation',\n",
    "                                               #color_mode = 'grayscale',\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff46fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "    \"\"\"\n",
    "    Compiles a model integrated with VGG16 pretrained layers\n",
    "    \n",
    "    input_shape: tuple - the shape of input images (width, height, channels)\n",
    "    n_classes: int - number of classes for the output layer\n",
    "    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n",
    "    fine_tune: int - The number of pre-trained layers to unfreeze.\n",
    "                If set to 0, all pretrained layers will freeze during training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
    "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
    "    # MobileNetV2(include_top=False,\n",
    "     #                weights='imagenet', \n",
    "      #               input_shape=)\n",
    "    \n",
    "    conv_base = tf.keras.applications.densenet.DenseNet121(\n",
    "    input_shape=input_shape, include_top=False, weights='imagenet'\n",
    ")\n",
    "    \n",
    "    # Defines how many layers to freeze during training.\n",
    "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
    "    # depending on the size of the fine-tuning parameter.\n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
    "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
    "    top_model = conv_base.output\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(1524, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.4)(top_model)\n",
    "    top_model = Dense(1524, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.4)(top_model)\n",
    "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
    "    \n",
    "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "\n",
    "    # Compiles the model for training.\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690f76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from adabound import AdaBound\n",
    "import tensorflow as tf\n",
    "input_shape = (48, 48, 3)\n",
    "\n",
    "n_classes=10\n",
    "optim_1 = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001\n",
    ")\n",
    "n_steps = traingen.samples // BATCH_SIZE\n",
    "n_val_steps = validgen.samples // BATCH_SIZE\n",
    "n_epochs = 60\n",
    "\n",
    "# First we'll train the model without Fine-tuning\n",
    "mobilenetv2_model = create_model(input_shape, n_classes, optim_1, fine_tune=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e0bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "327/327 [==============================] - 33s 90ms/step - loss: 1.9720 - accuracy: 0.3209 - val_loss: 1.6646 - val_accuracy: 0.4371\n",
      "Epoch 2/60\n",
      "327/327 [==============================] - 29s 88ms/step - loss: 1.7307 - accuracy: 0.3961 - val_loss: 1.5852 - val_accuracy: 0.4452\n",
      "Epoch 3/60\n",
      "327/327 [==============================] - 29s 89ms/step - loss: 1.6451 - accuracy: 0.4245 - val_loss: 1.4899 - val_accuracy: 0.4973\n",
      "Epoch 4/60\n",
      "327/327 [==============================] - 29s 90ms/step - loss: 1.5795 - accuracy: 0.4521 - val_loss: 1.5440 - val_accuracy: 0.4637\n",
      "Epoch 5/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.5616 - accuracy: 0.4591 - val_loss: 1.4649 - val_accuracy: 0.4992\n",
      "Epoch 6/60\n",
      "327/327 [==============================] - 30s 90ms/step - loss: 1.5014 - accuracy: 0.4804 - val_loss: 1.4510 - val_accuracy: 0.4969\n",
      "Epoch 7/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.4817 - accuracy: 0.4857 - val_loss: 1.4006 - val_accuracy: 0.5201\n",
      "Epoch 8/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.4614 - accuracy: 0.4912 - val_loss: 1.4359 - val_accuracy: 0.5131\n",
      "Epoch 9/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.4280 - accuracy: 0.5011 - val_loss: 1.3871 - val_accuracy: 0.5181\n",
      "Epoch 10/60\n",
      "327/327 [==============================] - 30s 90ms/step - loss: 1.4199 - accuracy: 0.5121 - val_loss: 1.3722 - val_accuracy: 0.5370\n",
      "Epoch 11/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.3956 - accuracy: 0.5120 - val_loss: 1.3439 - val_accuracy: 0.5421\n",
      "Epoch 12/60\n",
      "327/327 [==============================] - 30s 92ms/step - loss: 1.3738 - accuracy: 0.5257 - val_loss: 1.3533 - val_accuracy: 0.5451\n",
      "Epoch 13/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.3600 - accuracy: 0.5304 - val_loss: 1.3812 - val_accuracy: 0.5359\n",
      "Epoch 14/60\n",
      "327/327 [==============================] - 30s 92ms/step - loss: 1.3456 - accuracy: 0.5303 - val_loss: 1.3009 - val_accuracy: 0.5617\n",
      "Epoch 15/60\n",
      "327/327 [==============================] - 30s 90ms/step - loss: 1.3331 - accuracy: 0.5383 - val_loss: 1.3457 - val_accuracy: 0.5505\n",
      "Epoch 16/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.3134 - accuracy: 0.5524 - val_loss: 1.3676 - val_accuracy: 0.5285\n",
      "Epoch 17/60\n",
      "327/327 [==============================] - 29s 90ms/step - loss: 1.3236 - accuracy: 0.5436 - val_loss: 1.3286 - val_accuracy: 0.5606\n",
      "Epoch 18/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.2936 - accuracy: 0.5616 - val_loss: 1.3299 - val_accuracy: 0.5486\n",
      "Epoch 19/60\n",
      "327/327 [==============================] - 29s 89ms/step - loss: 1.2817 - accuracy: 0.5658 - val_loss: 1.3355 - val_accuracy: 0.5517\n",
      "Epoch 20/60\n",
      "327/327 [==============================] - 29s 88ms/step - loss: 1.2608 - accuracy: 0.5641 - val_loss: 1.2981 - val_accuracy: 0.5660\n",
      "Epoch 21/60\n",
      "327/327 [==============================] - 29s 88ms/step - loss: 1.2449 - accuracy: 0.5720 - val_loss: 1.3211 - val_accuracy: 0.5536\n",
      "Epoch 22/60\n",
      "327/327 [==============================] - 32s 98ms/step - loss: 1.2357 - accuracy: 0.5817 - val_loss: 1.2729 - val_accuracy: 0.5721\n",
      "Epoch 23/60\n",
      "327/327 [==============================] - 31s 95ms/step - loss: 1.2215 - accuracy: 0.5761 - val_loss: 1.2824 - val_accuracy: 0.5687\n",
      "Epoch 24/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.2332 - accuracy: 0.5807 - val_loss: 1.3094 - val_accuracy: 0.5532\n",
      "Epoch 25/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.2043 - accuracy: 0.5926 - val_loss: 1.3016 - val_accuracy: 0.5721\n",
      "Epoch 26/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.2179 - accuracy: 0.5826 - val_loss: 1.2872 - val_accuracy: 0.5729\n",
      "Epoch 27/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.2013 - accuracy: 0.5905 - val_loss: 1.2654 - val_accuracy: 0.5799\n",
      "Epoch 28/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1758 - accuracy: 0.5915 - val_loss: 1.2345 - val_accuracy: 0.5926\n",
      "Epoch 29/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1859 - accuracy: 0.5884 - val_loss: 1.2514 - val_accuracy: 0.6007\n",
      "Epoch 30/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1654 - accuracy: 0.6006 - val_loss: 1.2349 - val_accuracy: 0.5772\n",
      "Epoch 31/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1691 - accuracy: 0.6001 - val_loss: 1.2438 - val_accuracy: 0.5876\n",
      "Epoch 32/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1450 - accuracy: 0.6099 - val_loss: 1.2879 - val_accuracy: 0.5664\n",
      "Epoch 33/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1520 - accuracy: 0.6144 - val_loss: 1.2289 - val_accuracy: 0.5895\n",
      "Epoch 34/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1468 - accuracy: 0.6164 - val_loss: 1.2502 - val_accuracy: 0.5914\n",
      "Epoch 35/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1502 - accuracy: 0.6053 - val_loss: 1.2361 - val_accuracy: 0.5972\n",
      "Epoch 36/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1217 - accuracy: 0.6165 - val_loss: 1.2432 - val_accuracy: 0.5876\n",
      "Epoch 37/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1298 - accuracy: 0.6122 - val_loss: 1.2415 - val_accuracy: 0.5764\n",
      "Epoch 38/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1065 - accuracy: 0.6255 - val_loss: 1.2419 - val_accuracy: 0.5907\n",
      "Epoch 39/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.1057 - accuracy: 0.6204 - val_loss: 1.2215 - val_accuracy: 0.6015\n",
      "Epoch 40/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0950 - accuracy: 0.6324 - val_loss: 1.2110 - val_accuracy: 0.6007\n",
      "Epoch 41/60\n",
      "327/327 [==============================] - 30s 92ms/step - loss: 1.0787 - accuracy: 0.6320 - val_loss: 1.2251 - val_accuracy: 0.5934\n",
      "Epoch 42/60\n",
      "327/327 [==============================] - 30s 92ms/step - loss: 1.0856 - accuracy: 0.6292 - val_loss: 1.2249 - val_accuracy: 0.5849\n",
      "Epoch 43/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0885 - accuracy: 0.6284 - val_loss: 1.2124 - val_accuracy: 0.5972\n",
      "Epoch 44/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0714 - accuracy: 0.6298 - val_loss: 1.2070 - val_accuracy: 0.5930\n",
      "Epoch 45/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0693 - accuracy: 0.6386 - val_loss: 1.2153 - val_accuracy: 0.5949\n",
      "Epoch 46/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0599 - accuracy: 0.6409 - val_loss: 1.2189 - val_accuracy: 0.6123\n",
      "Epoch 47/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0604 - accuracy: 0.6409 - val_loss: 1.2162 - val_accuracy: 0.6100\n",
      "Epoch 48/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0549 - accuracy: 0.6440 - val_loss: 1.2197 - val_accuracy: 0.5914\n",
      "Epoch 49/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0573 - accuracy: 0.6341 - val_loss: 1.1920 - val_accuracy: 0.6103\n",
      "Epoch 50/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0665 - accuracy: 0.6430 - val_loss: 1.1985 - val_accuracy: 0.6073\n",
      "Epoch 51/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0505 - accuracy: 0.6444 - val_loss: 1.2294 - val_accuracy: 0.5899\n",
      "Epoch 52/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0403 - accuracy: 0.6460 - val_loss: 1.1813 - val_accuracy: 0.5934\n",
      "Epoch 53/60\n",
      "327/327 [==============================] - 31s 94ms/step - loss: 1.0281 - accuracy: 0.6513 - val_loss: 1.1868 - val_accuracy: 0.6073\n",
      "Epoch 54/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0395 - accuracy: 0.6543 - val_loss: 1.1864 - val_accuracy: 0.6053\n",
      "Epoch 55/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0118 - accuracy: 0.6556 - val_loss: 1.1831 - val_accuracy: 0.6069\n",
      "Epoch 56/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0035 - accuracy: 0.6610 - val_loss: 1.1989 - val_accuracy: 0.6049\n",
      "Epoch 57/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0231 - accuracy: 0.6555 - val_loss: 1.2207 - val_accuracy: 0.5999\n",
      "Epoch 58/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0222 - accuracy: 0.6542 - val_loss: 1.1577 - val_accuracy: 0.6107\n",
      "Epoch 59/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0120 - accuracy: 0.6632 - val_loss: 1.1929 - val_accuracy: 0.6069\n",
      "Epoch 60/60\n",
      "327/327 [==============================] - 30s 91ms/step - loss: 1.0216 - accuracy: 0.6571 - val_loss: 1.1702 - val_accuracy: 0.6177\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mobilenetv2_model = mobilenetv2_model.fit(traingen,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=validgen,\n",
    "                            steps_per_epoch=n_steps,\n",
    "                            validation_steps=n_val_steps,\n",
    "                           # callbacks=[tl_checkpoint_1, early_stop, plot_losses],\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb5234-406c-4370-8b48-f464b7341e56",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3e93b-1785-4f6a-be1b-48a5606f1382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb204f-9143-495c-a413-6bd41e105f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae11861-2d17-42b5-bc05-ba3c446f5a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
